---
sidebar_position: 3
title: Automate network policies
---
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

Otterize automates network policies within your cluster by abstracting away
the management of pod identities, the labeling of clients, servers and namespaces,
and the manual authoring of individual network policies.

With intent-based access control, you just declare what client calls will be needed,
and everything is automatically wired together so only intended calls are allowed.

In this tutorial, we will

- Install Otterize in your Kubernetes cluster.
- Make sure your Kubernetes cluster enforces network policies
- Deploy a server pod and a client pod that calls it
- Deploy a "default deny" network policy to block undeclared calls
- Declare that the client pod intends to call the server pod
- See that a network policy was autogenerated to allow just that

## Install Otterize

:::note
You can skip this section if Otterize is already installed in your cluster.
:::

1. Use Helm to install the latest version of Otterize:
   ```shell
   helm repo add otterize https://otterize.github.io/helm-charts
   helm repo update
   helm upgrade --install otterize otterize/otterize-kubernetes -n otterize --create-namespace
   ```
2. It can take several minutes for the pods to be `Running` and all containers to be ready.
   You can monitor progress with the following command:
   ```
   kubectl get pods -n otterize -w
   ```
   Once you see the following (there may be even more pods), you can stop monitoring with `Ctrl-C`:
   ```bash
   NAME                                                             READY   STATUS    RESTARTS      AGE
   intents-operator-controller-manager-6b97596d54-5qxcw             2/2     Running   0             53s
   otterize-spire-agent-9s8w7                                       1/1     Running   0             54s
   otterize-spire-agent-np2wf                                       1/1     Running   1             54s
   otterize-spire-server-0                                          1/1     Running   0             53s
   otterize-watcher-77db87cfcd-xhsrk                                1/1     Running   0             53s
   spire-integration-operator-controller-manager-65b8bf57b5-mpltl   2/2     Running   0             53s
   ```

## Verify network policies are enforceable

To enforce network policies, a Kubernetes cluster requires a CNI network plugin to be installed.

:::note
You can skip this section if you already have a CNI (such as Calico) installed in your cluster.
:::

If you don't have a CNI installed, a popular choice is Calico by Tigera.
To install it, please follow the [instructions](https://projectcalico.docs.tigera.io/getting-started/kubernetes/helm)
and return to this tutorial.

## Deploy the server, client, and default network policy

Our simple example consists of two pods: an HTTP server and a client that calls it.

We also deploy a default-deny ingress network policy,
blocking pods from accepting incoming calls unless another network policy explicitly allows them.

<details>
<summary>Expand to see the details of this example...</summary>
<Tabs>

<TabItem value="namespace.yaml" label="namespace.yaml" default>

   ```yaml
   apiVersion: v1
   kind: Namespace
   metadata:
     name: otterize-tutorial-npol
   ```

</TabItem>

<TabItem value="server.yaml" label="server.yaml" default>

   ```yaml
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: server
     namespace: otterize-tutorial-npol
   spec:
     selector:
       matchLabels:
         app: server
     template:
       metadata:
         labels:
           app: server
       spec:
         containers:
           - name: server
             image: hashicorp/http-echo
             args: [ "-listen=:80", "-text=Hi, I am the server, you called, may I help you?" ]
   ```

</TabItem>
<TabItem value="client.yaml" label="client.yaml" default>

   ```yaml
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: client
     namespace: otterize-tutorial-npol
   spec:
     selector:
       matchLabels:
         app: client
     template:
       metadata:
         labels:
           app: client
       spec:
         containers:
           - name: client
             image: alpine/curl
             command: [ "/bin/sh", "-c", "--" ]
             args: [ "while true; do echo \"Calling server...\"; if ! timeout 2 curl -si server 2>/dev/null; then echo \"curl timed out\"; fi; sleep 2; done" ]
   ```

</TabItem>

<TabItem value="default-deny.yaml" label="default-deny.yaml" default>

   ```yaml
   apiVersion: networking.k8s.io/v1
   kind: NetworkPolicy
   metadata:
     name: default-deny-ingress
     namespace: otterize-tutorial-npol
   spec:
     podSelector: { }
     policyTypes:
       - Ingress
   ```

</TabItem>
</Tabs>
</details>

1. Deploy the client, server, and the default deny network policy using `kubectl`.

   ```shell
   kubectl apply -f https://docs.otterize.com/code-examples/network-policies/all.yaml
   ```
2. Check that the `client` and server `pods` were deployed
   ```bash
   kubectl get pods -n otterize-tutorial-npol
   ```
   You should see
   ```
   NAME                      READY   STATUS    RESTARTS   AGE
   client-5689997b5c-grlnt   1/1     Running   0          35s
   server-6698c58cbc-v9n9b   1/1     Running   0          34s
   ```
3. Check that the default deny ingress network policy was deployed:
   ```bash
   kubectl get NetworkPolicies -n otterize-tutorial-npol
   ```
   You should see
   ```
   NAME                   POD-SELECTOR   AGE
   default-deny-ingress   <none>         17s
   ```
4. Let's monitor the client's attempts to call the server with a second terminal window,
   so we can see the effects of our changes in real time.

   **Open a second terminal window** and tail the client log:
   ```bash
   kubectl logs -f --tail 1 -n otterize-tutorial-npol deploy/client
   ```
   At this point the client should be timing out when trying to call the server:
   ```
   Calling server...
   curl timed out
   Calling server...
   curl timed out
   Calling server...
   curl timed out
   ```
   This is the expected outcome since by default all pods' ingress are blocked by the default network policy.
   Once the client declares its intents, Otterize will add a network policy to allow thhe intended calls.
   Let's see that now...

## Apply intents

1. The client declares its intent to call the server with this `intents.yaml` file:
   ```yaml
   apiVersion: k8s.otterize.com/v1
   kind: ClientIntents
   metadata:
     name: client
     namespace:  otterize-tutorial-npol
   spec:
     service:
       name: client
     calls:
       - name: server
         type: HTTP
   ```
   :::tip
   Client intents are the cornerstone of [intent-based access control](otterize.com/ibac).
   :::

   Keep an eye on the logs being tailed in the **second terminal window**
   while you apply this `intents.yaml` file in your **main terminal window** using:
   ```shell
   kubectl apply -f https://docs.otterize.com/code-examples/network-policies/intents/intents.yaml
   ```
2. You should quickly see in the **second terminal** that the client is now successfully calling the server:
   ```bash
   Calling server...
   curl timed out
   Calling server...
   curl timed out                              <- before applying the intents file
   # highlight-start
   Calling server...                           <- after applying the intents file
   HTTP/1.1 200 OK
   X-App-Name: http-echo
   X-App-Version: 0.2.3
   Date: Wed, 07 Sep 2022 13:51:34 GMT
   Content-Length: 12
   Content-Type: text/plain; charset=utf-8
   
   Hi, I am the server, you called, may I help you?
   # highlight-end
   ```

3. If you check, you should see that a new network policy was created
   ```bash
   kubectl get NetworkPolicies -n otterize-tutorial-npol
   ```
   You should see
   ```
   NAME                                           POD-SELECTOR                                         AGE
   access-to-server-from-otterize-tutorial-npol   otterize/server=server-otterize-tutorial-np-7e16db   6s
   default-deny-ingress                           <none>                                               28s
   ```

:::tip
To learn more about how Otterize + network policies work see
details [here](/documentation/intents-operator/network-policies/in-depth)
:::

## What happened behind the scenes

To generate network policies between two pods, for example called `client` and `server` running in
namespace `otterize-tutorial-npol` we needed to:

1. Label the `server` pod `label-server` - we will use this label in the network policy to apply it to all pods with
   this label (as an ingress rule).
2. Label the `client` pod with `has-access-to-server`, saying that this specific pod has access to `server` - we will
   use this label in the network policy as a filter -- every
   pod that has the label saying [this pod can access the server] will be able to pass through the network policy.
3. Label the `client`'s namespace `label-otterize-tutorial-npol` -- this is a requirement for the network policy as
   another filtering mechanism for pods.
4. Generate a network policy saying that
    1. Only pod with the label `has-access-to-server`
    2. From the namespaces with the label `label-otterize-tutorial-npol`
    3. Can access pods with the label `label-server`

Otterize saved us from doing all this work by simply declaring the client's intents in `intents.yaml`,
while the appropriate network policies were managed automatically behind the scenes.

## What's next

<!-- [Intents Operator](/documentation/intents-operator): -->

- Configure [network policies](/documentation/intents-operator/network-policies) for your existing deployments.
- Explore the [Network Mapper](/documentation/getting-started/network-mapper) to help you bootstrap intents you can
  apply as network policies.

## Teardown

To remove the deployed resources run

```bash
kubectl delete namespace otterize-tutorial-npol
```