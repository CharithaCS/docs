---
sidebar_position: 2
title: Map your cluster
---
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

The network mapper allows you to map pod-to-pod traffic within your K8s cluster. This tutorial will guide you
through installing Otterize, mapping traffic and tracking changes.

## Install the network mapper

{@include: ../_common/install-otterize-network-mapper.md}

## Install the Otterize CLI

{@include: ../_common/install-otterize-cli.md}

## Map the cluster

The network mapper starts to sniff traffic and build an in-memory network map as soon as it's installed.

### Deploy demo to simulate traffic

Let's add traffic to the cluster and see how the network mapper tracks it.
Deploy the following simple example &mdash; `client`, `client2` and `server`, communicating over HTTP:
```shell
kubectl apply -n otterize-tutorial-mapper -f https://docs.otterize.com/code-examples/network-mapper/all.yaml
```

### Show mapped traffic

You can view mapped traffic by calling the CLI `list` or `export` commands.
The latter supports multiple output formats, namely `intents` (client intents files) and `json`.
The following example shows the CLI output filtered for the namespace (`otterize-tutorial-mapper`)
of the example above.

<Tabs>
  <TabItem value="plain" label="Plain" default>

1. List the pod-to-pod network map built up ("sniffed") so far:

   ```shell
   otterize mapper list -n otterize-tutorial-mapper
   ```
2. For the simple example above, you should see:
   ```shell
   client in namespace otterize-tutorial-mapper calls:
     - server
   client2 in namespace otterize-tutorial-mapper calls:
     - server
   ```

</TabItem>
  <TabItem value="intents" label="Intents" default>

1. Export as YAML client intents (the default format) the pod-to-pod network map built up so far:

   ```shell
   otterize mapper export -n otterize-tutorial-mapper
   ```
2. For the simple example above, you should see (concatenated into one YAML file):
   ```yaml
   apiVersion: k8s.k8s.otterize.com/v1
   kind: ClientIntents
   metadata:
     name: client
     namespace: otterize-tutorial-mapper
   spec:
     service:
       name: client
     calls:
       - name: server
   ---
   apiVersion: k8s.k8s.otterize.com/v1
   kind: ClientIntents
   metadata:
     name: server
     namespace: otterize-tutorial-mapper
   spec:
     service:
       name: client2
     calls:
       - name: server
   ```

</TabItem>
  <TabItem value="json" label="JSON">

1. Export as JSON the pod-to-pod network map built up so far as JSON:
   ```shell
   otterize mapper export -n otterize-tutorial-mapper --format json
   ```
2. For the simple example above, you should see:

   ```json
   [
     {
       "kind": "ClientIntents",
       "apiVersion": "k8s.otterize.com/v1alpha1",
       "metadata": {
         "name": "client",
         "namespace": "otterize-tutorial-mapper"
       },
       "spec": {
         "service": {
           "name": "client"
         },
         "calls": [
           {
             "name": "server"
           }
         ]
       }
     },
     {
       "kind": "ClientIntents",
       "apiVersion": "k8s.otterize.com/v1alpha1",
       "metadata": {
         "name": "client",
         "namespace": "otterize-tutorial-mapper"
       },
       "spec": {
         "service": {
           "name": "client"
         },
         "calls": [
           {
             "name": "server"
           }
         ]
       }
     }
   ]
   ```

</TabItem>
</Tabs>

:::info
See the [CLI documentation](/cli) for more details about the CLI.
:::

### Change traffic, then "Sniff&Diff"

One of the benefits for using the network mapper is the ability to track changes over time for communication within your
cluster. We call that "Sniff&Diff": continue to sniff traffic, then diff the network mapper output to see what changed.

1. Let's save the current state of traffic from the cluster into a file we will compare against later:
   ```shell
   otterize mapper list -n otterize-tutorial-mapper > intents-original.txt
   ```

2. And now we can add more traffic to the cluster and see how the network mapper tracks it.
   Let's deploy an extension to the example, consisting of two pods &mdash;
   `other-client` and `other-server`, communicating over HTTP:

   ```shell
   kubectl apply -n otterize-tutorial-mapper -f https://docs.otterize.com/code-examples/network-mapper/all-other.yaml
   ```

<details>
<summary>Check that the client and server pods were deployed</summary>

```bash
kubectl get pods -n otterize-tutorial-mapper
```
You should see
```
NAME                      READY   STATUS    RESTARTS   AGE
client-756f7677f8-rdm8l         1/1     Running   0          15m
client2-5c4479947b-cdkl2        1/1     Running   0          9m18s
other-client-8666d97d9c-29xpg   1/1     Running   0          4m8s
other-server-676dbc5f5-bqxs4    1/1     Running   0          4m3s
server-6698c58cbc-tht9n         1/1     Running   0          15m
```
</details>

3. Let's list the network map again:
   ```shell
   otterize mapper list -n otterize-tutorial-mapper
   ```
   You should see all the calls including the new ones:
   ```shell
   client in namespace otterize-tutorial-mapper calls:
     - server
   client2 in namespace otterize-tutorial-mapper calls:
     - server
   # highlight-start
   other-client in namespace otterize-tutorial-mapper calls:
     - other-server
   # highlight-end
      ```
5. We can also compare both outputs to see the difference. Start by saving the updated state to a file:
   ```bash
   otterize mapper list -n otterize-tutorial-mapper > intents-updated.txt
   ```
6. Now compare the original file with the updated file using:
   ```bash
   diff -y intents-original.txt intents-updated.txt;echo
   ```
   Note the new calls identified on the lower right:
   ```bash
   client in namespace otterize-tutorial-mapper calls:             client in namespace otterize-tutorial-mapper calls:
     - server                                                        - server
   client2 in namespace otterize-tutorial-mapper calls:            client2 in namespace otterize-tutorial-mapper calls:
     - server                                                        - server
   # highlight-start
                                                                  > other-client in namespace otterize-tutorial-mapper calls:
                                                                  >   - other-server
   # highlight-end
      ```

## What's next

One of the outputs of the network mapper is a set of intents that reflect the observed traffic.
The mapper doesn't look at the types and contents of the calls, so the intents won't have granular information
beyond what service calls what service. But it's a pretty good way to bootstrap intents files and get
going with intent-based access control.

Take a look at the **intents** tab in the section called [Show mapped traffic](#show-mapped-traffic).
With these intents we can then automatically control pod-to-pod access using Kubernetes network policies:
in other words, these intents create network policies that reflect the mapped traffic, and as you
evolve the intents and apply them, the network policies will always reflect the intents.

Where to go next?

- If you haven't already, see the [automate network policies tutorial](/quick-tutorials/k8s-network-policies).
- Or go to the next tutorial to [automate secure access for Kafka](/quick-tutorials/kafka-mtls).
- You might also dive deeper into the [IBAC with network policies guide](../guides/k8s-ibac-via-network-policies).
  - It even contains a bootstrapping exercise at the end.
- For a deeper dive into the network mapper, see the [Mapping pod-to-pod calls guide](../guides/k8s-mapping-pod-to-pod-calls).

### Teardown

To remove the deployed resources run:

```bash
kubectl delete namespace otterize-tutorial-mapper
helm network-mapper uninstall -n otterize-system
```
